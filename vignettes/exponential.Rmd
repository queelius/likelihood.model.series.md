---
title: "Series system with exponentially distributed component lifetimes"
output:
    rmarkdown::html_vignette:
        toc: true
vignette: >
  %\VignetteIndexEntry{Series system with exponentially distributed component lifetimes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>")

library(series.system.estimation.masked.data)
library(algebraic.mle)
library(md.tools)
library(tidyverse)
library(devtools)
library(printr)

options(digits=3)
```


\renewcommand{\v}[1]{\boldsymbol{#1}}

Introduction
============

The R package `series.system.estimation.masked.data` is a framework for
estimating the parameters of latent component lifetimes from *masked data*
in a series system.

# Masked data

Masked data is given by an i.i.d. sample of system lifetime data and
*plausible* candidate sets that contain the failed node.

The R package `series.system.estimation.masked.data` contains several simulated
masked data sets.
For example, a series system with $3$ exponentially distributed component
lifetimes is stored in `exp_series_data_1`:

```{r}
print(exp_series_md_1)
```

You can get help on any object in `series.system.estimation.masked.data` using
the built-in help.
For instance, to get information on the data set `exp_series_md_1`, type
`?exp_series_md_1` in your R console.

# Statistical model for masked data

The principle object of study is the series system consisting of $m$
components. We are interested in estimating the component lifetimes from
masked data in the form of candidate sets, where the candidate sets
satisfy the following set of conditions:
    
- Condition $C_1$: The index of the failed component is in the candidate set,
  i.e., $\Pr\{K_i \in C_i\} = 1$.

- Condition $C_2$: The probability of $C_i$ given $K_i$ and $T_i$ is equally
  probable when the failed component varies over the components in the candidate
  set, i.e., $\Pr\{C_i=c_i|K_i=j,T_i=t_i\} = \Pr\{C_i=c_i|K_i=j',T_i=t_i\}$ for
  any $j,j' \in c_i$.
    
- Condition $C_3$: The masking probabilities are independent of $\v\theta$,
  i.e., $\Pr\{C_i=c_i|K_i=j,T_i=t_i\}$ is not a function of $\v\theta$.
  
  That means, whatever the generative mechanism underlying
  $\mathcal{C}_1,\ldots,\mathcal{C}_n$, it has no explicit knowledge of $\v\theta$,
  but the $i$\textsuperscript{th} candidate set $\mathcal{C}_i$ may depend on
  the realizations of $T_{i 1},\ldots,T_{i n}$ and other factors not explicitly
  in the statistical model we have described.


## Bernoulli candidate model that satisfies $C_1$, $C_2$, and $C_3$

As long as we satisfy conditions $C_1$, $C_2$, and $C_3$, our reduced
likelihood function that assumes those conditions obtains the same MLEs 
as the full likelihood function.

In what follows, we describe our Bernoulli candidate set model that generates
candidate sets that satisfy these conditions.\footnote{There are many other
ways to generate candidate sets that satisfy these conditions, and they
may generate masked data that is more or less informative (e.g., the
same MLEs but with different MSEs).}

```{r,warning=F,message=F,echo=F}
?md_bernoulli_candidate_C1_C2_C3
md_bernoulli_candidate_C1_C2_C3
```

## Estimating the variance-covariance using the Bootstrap method
Alternatively, we could estimate $\v\theta$ with $B$ simulated draws from
the MLEs that satisfy
$$
    \operatorname{argmax}_{\v\theta \in \v\Omega} \ell(\v\theta|\mathcal{D_i})
$$
where $\mathcal{\v{D_i}}$ is a random sample from the empirical distribution
$\{(S_i,\delta_i,C_i)\}_1^n$. We call this the *Bootstrap*.

Assuming the above solution to the MLE equation is _unique_, this gives us a
single point $\hat{\v\theta}_{(i)}$ when conditioned on the simulated masked
data $\v{D_i}$.


Exponential series system
=========================

The most straightforward series system to estimate is the series system with
exponentially distributed component lifetimes.

Suppose an exponential series system with $m$ components is parameterized by
$\v\theta = (1,1.25,1.5)'$.
Then, the component assigned to index $j$ has an exponentially distributed
lifetime with a failure rate $\theta_j$, e.g., $\theta_2 = 1.25$ is the failure
rate of the component indexed by $2$.

Let's simulate a series system (without candidate sets):
```{r}
n <- 300
theta <- c(1,1.25,1.5)
m <- length(theta)

md <- tibble(t1=rexp(n,theta[1]),
             t2=rexp(n,theta[2]),
             t3=rexp(n,theta[3])) %>%
    md_series_lifetime()
print(md)
```


In the above, we used the function `md_series_lifetime`.
To get more help on it, type `?md_series_lifetime`.

## Candidate sets
We simulate candidate sets using the Bernoulli candidate model with an
appropriate set of parameters to satisfy conditions $C_1$, $C_2$, and $C_3$:
```{r warning=F, message=F}
md <- md %>% md_bernoulli_candidate_C1_C2_C3(m, function(n) rep(.333,n))
print(md)
```

## Log-likelihood of $\theta$ given masked data

The reduced log-likelihood function (the log of the kernel of the likelihood
function) is given by
$$
\ell(\theta) =
    -\left(\sum_{i=1}^{n} t_i\right)
    \left(\sum_{j=1}^{m} \theta_j\right) +
    \sum_{i=1}^{n} \log\left(\sum_{j \in c_i} \theta_j\right).
$$

The following log-likelihood constructor, `md_loglike_exp_series_C1_C2_c3`,
is implemented using minimally sufficient statistics, which significantly
improves the computational efficiency of computing the log-likelihood.

We compute the log-likelihood function as a function of the masked data `md` with:
```{r}
l <- md_loglike_exp_series_C1_C2_C3(md)
```

The log-likelihood function contains the maximum amount of information
about parameter $\v\theta$ given the sample of masked data `md` satisfying
conditions $C_1$, $C_2$, and $C_3$.

Suppose we do not know that $\v\theta = (3,4,5)'$.
With the log-likelihood, we may estimate $\theta$ with $\hat\theta$ by solving
$$
\hat{\v\theta} = \operatorname{argmax}_{\v\theta \in \Omega} \ell(\theta),
$$
i.e., finding the point that *maximizes* the log-likelihood on
the observed sample `md`.
This is known as *maximum likelihood estimation* (MLE).
We typically solve for the MLE by solving
$$
\nabla \ell|_{\v\theta=\hat{\v\theta}} = \v{0}.
$$
We use the iterative method known as the gradient ascent to solve this,
$$
\v\theta^{(n+1)} = \v\theta^n + \alpha_n \nabla \ell(\v\theta^n),
$$
where $\alpha_n$ is chosen to approximately maximize $\ell(\theta^{(n+1))}$ by
using backtracking line search.

We use the function `mle_gradient_ascent` provided by the R package `algebraic.mle`
with the appropriate arguments.
We find $\hat{\v\theta}$ by running the following R code:
```{r}
scr <- md_score_exp_series_C1_C2_C3(md)
mle <- mle_gradient_ascent(l=l,theta0=theta,score=scr)
theta.hat <- point(mle)
summary(mle)
```

The function `md_gradient_ascent` returns an `mle` object, which
has various methods implemented for it, e.g., `confint` (computes the
estimators confidence interval).
In the above, we see use of the `summary` method, which takes an `mle`
object and prints out a summary of its properties.
We let `theta.hat` be given by the `point` method, which obtains the point
$\hat{\v\theta}$.

We see that $\hat{\v\theta} = (`r theta.hat`)$.
If we let the third argument in the log-likelihood function be fixed
at $\hat\theta_3 = `r theta.hat[3]`)$, then we may profile the
log-likelihood function over the first two parameters:

```{r,message=F}
prof <- function(theta1) { l(c(theta1,theta.hat[2],theta.hat[3])) }
data <- tibble(x=seq(theta.hat[1]-2,theta.hat[1]+2,.05))
data$y <- numeric(nrow(data))
for (i in 1:nrow(data))
    data$y[i] <- prof(data$x[i])
data %>% ggplot(aes(x=x,y=y)) + geom_line() +
    geom_point(aes(x=theta.hat[1],prof(theta.hat[1])))
```

Due to sampling variability, different runs of the experiment
will result in different outcomes, i.e., $\hat{\v\theta}$ has a
sampling distribution.
We see that $\hat{\v\theta} \neq \v\theta$, but it is reasonably
close.
We may measure this sampling variability using the variance-covariance
matrix, bias, mean squared error (MSE), and confidence intervals.

## Sampling distribution of the MLE
The MLE $\hat{\v\theta}$ as a function of a random sample
of masked data, and is thus a random vector.
Theoretically, $\hat{\v\theta}$ converges in distribution
to the multivariate normal with a mean $\v\theta$ and we
may estimate the variance-covariance with the inverse of the
observed Fisher matrix, which is given by
$$
    J(\hat{\v\theta}) = -\nabla^2 l|_{\hat{\v\theta}}.
$$
Thus,
$$
    \hat{\v\theta} \sim \mathcal{N}(\v\theta,J^{-1}(\hat{\v\theta})).
$$

Asymptotically, $\hat{\v\theta}$ is the UMVUE, i.e.,
it is unbiased and obtains the minimum sampling variance.
An estimate of the variance-covariance may be obtained with:
```{r}
(V.hat <- vcov(mle))
```

## Bias and mean squared error
We would like to measure the accuracy and precision of $\hat{\v\theta}$.
In statistical literature, the bias
$$
\operatorname{b}(\hat{\v\theta}) = E(\hat{\v\theta}) - \v\theta
$$
is a measure of accuracy and variance is a measure of precision.

The mean squared error, denoted by $\operatorname{MSE}$, is a measure of
estimator error that incorporates both the bias and the variance,
$$
\operatorname{MSE}(\hat{\v\theta}) =
    \operatorname{trace}\bigl(\operatorname{vcov}(\hat{\v\theta})\bigr) +
    \operatorname{b}^2(\hat{\v\theta}).
$$

Since $\hat{\v\theta}$ is asymptotically unbiased and minimum variance,
$$
\lim_{n \to \infty} \operatorname{MSE}(\hat{\v\theta}) =
    \operatorname{trace}\bigl(\operatorname{vcov}(\hat{\v\theta})\bigr).
$$
Thus, for sufficiently large samples, $\operatorname{MSE}(\hat{\v\theta})$ is
approximately given by the `trace` of the estimated variance-covariance matrix:
```{r}
(mse <- sum(diag(V.hat)))
```

If we have a sample of $n$ MLEs, $\hat{\v\theta}^{(1)},\ldots,\hat{\v\theta}^{(n)}$,
then we may estimate both the bias and the MSE respectively with the statistics
$$
\hat{\operatorname{b}} = \frac{1}{n} \sum_{i=1} \hat{\v\theta}^{(i)} - \v\theta
$$
and
$$
\widehat{\operatorname{MSE}} = \frac{1}{n}
    \sum_{i=1}^n (\hat{\v\theta}^{(i)} - \v\theta)
                 (\hat{\v\theta}^{(i)} - \v\theta)'.
$$
We may then compare these statistics, $\hat{\operatorname{b}}$ and
$\widehat{\operatorname{MSE}}$, with the asymptotic bias $(\v{0})$ and the
asymptotic $\operatorname{MSE}$.

Let us compute estimates of the bias, variance-covariance, and mean squared
error as a function of sample size $n$ using Monte Carlo simulation.
Note that this is similiar to the Bootstrap, except we know $\v\theta$.

```{r, fig.width=5,fig.height=5,echo=F}
# bias of mle
# variance-covariance of asymptotic: vcov(mle)
# variance-covariance estimator: cov(mle.boot)
# mse of mle
# bootstrap sampling distribution of mle: mle.boot

N <- c(100, 200) #, 400, 800, 1600, 3200, 6400, 12800, 256000)
for (ni in N)
{
    R <- ni
    data <- tibble(
        t1=rexp(ni,theta[1]),
        t2=rexp(ni,theta[2]),
        t3=rexp(ni,theta[3])) %>%
        md_series_lifetime() %>%
        md_bernoulli_candidate_C1_C2_C3(m=m,p=function(n) rep(.333,n))
    
    print(data)

    loglik <- md_loglike_exp_series_C1_C2_C3(data)
    scr <- md_score_exp_series_C1_C2_C3(data)
    theta.hat <- mle_gradient_ascent(l=loglik,theta0=theta,score=scr)
    summary(theta.hat)
    loglik.gen <- function(md) md_loglike_exp_series_C1_C2_C3(md)
    #theta.boot <- mle_samp_bs_loglike(x=theta.hat,loglike=loglik.gen,data=data,R=R)

    #summary(theta.boot)
    #mse(x=theta.boot,par=rates)
    #bias(x=theta.boot,par=rates)
    #point(theta.boot)
    #vcov(theta.boot)
    #confint(theta.boot)
}
knitr::knit_exit()
```


A primary statistic is the *confidence interval*.
A $(1-\alpha)100\%$ confidence interval for $\theta_j$ may be estimated with
$\hat\theta_j \pm z_{1-\alpha/2} \sqrt{\hat{V}_{j j}}$.
We provide a method for doing this calculation:
```{r}
as_tibble(confint(mle)) %>% mutate(length=.[[2]]-.[[1]])
```

How does this compare to the confidence intervals given that the candidate sets
are generated using the Bernoulli candidate model with a different
choice of parameters? First, we generate a new MLE using a different sample
of masked data, `new.md`:

```{r}
new.md <- tibble(
    t1=rexp(n=n,rate=theta[1]),
    t2=rexp(n=n,rate=theta[2]),
    t3=rexp(n=n,rate=theta[3])) %>%
    md_series_lifetime() %>%
    md_bernoulli_candidate_C1_C2_C3(m=3,p=function(n) rep(.333,n))

l <- md_loglike_exp_series_C1_C2_C3(new.md)
scr <- md_score_exp_series_C1_C2_C3(new.md)
mle.new <- mle_gradient_ascent(l=l,theta0=theta,score=scr)
```

Let's compare the lengths of the confidence intervals:
```{r}
as_tibble(confint(mle)) %>% mutate(length=.[[2]]-.[[1]])
as_tibble(confint(mle.new)) %>% mutate(length=.[[2]]-.[[1]])
```

We see that the lengths of the confidence intervals are significantly shorter.

If *no* information is provided about the component cause of failure
in a series system with $m$ components, then the $m_0$ estimator is not unique
and does not converge to $\v\theta$.

### Sampling distribution of the MLE

We know that
$$
\hat{\v\theta} \sim \mathcal{N}\bigl(\v\theta,J^{-1}(\v\theta)\bigr).
$$
We can estimate the sampling distribution of $\hat{\v\theta}$
with $\mathcal{N}\bigl(\hat\theta,J^{-1}(\hat{\v\theta})\bigr)$.
This makes it trivial to estimate any other function of
$\v\theta$ by sampling from the approximation:


In Figure 2, we show contour plots of the first two components for the MLE sample.

```{r fig2, eval=F,echo=F, fig.align='center',fig.cap="Figure 2: Countour plots of the first two components of the 500 MLEs."}
library(latex2exp)
library(patchwork)
library(mvtnorm)

fig2.N <- 500
fig2.theta.hats <- matrix(nrow=fig2.N,ncol=3)

for (i in 1:fig2.N)
{
    fig2.md <- tibble(t1=rexp(n=n,rate=theta[1]),
                      t2=rexp(n=n,rate=theta[2]),
                      t3=rexp(n=n,rate=theta[3])) %>%
    md_series_lifetime() %>%
    md_bernoulli_candidate_C1_C2_C3(m=3,p=function(n) rep(.5,n))
    fig2.theta.hats[i,] <- point(md_mle_exp_series_C1_C2_C3(md=fig2.md,theta0=theta))
}

fig2.plot1 <- ggplot(tibble(x=fig2.theta.hats[,1],y=fig2.theta.hats[,2]),aes(x=x,y=y)) +
    xlim(c(2,4)) + ylim(c(2,4)) +
    ggtitle("The empirical distribution.") +
    geom_density_2d() +
    xlab(TeX("$\\hat{\\lambda}_1$")) +
    ylab(TeX("$\\hat{\\lambda}_2$"))


fig2.vcov <- vcov(md_mle_exp_series_C1_C2_C3(md,theta))
fig2.theta.hat.asym <- rmvnorm(fig2.N,theta,fig2.vcov)

fig2.plot2 <- ggplot(tibble(x=fig2.theta.hat.asym[,1],y=fig2.theta.hat.asym[,2]),aes(x=x,y=y)) +
    xlim(c(2,4)) + ylim(c(2,4)) +
    ggtitle("The theoretical \n asymptotic distribution.") +
    geom_density_2d() +
    xlab(TeX("$\\hat{\\lambda}_1$")) +
    ylab(TeX("$\\hat{\\lambda}_2$"))

fig2.plot1+fig2.plot2
```

Estimating component cause
==========================
Another characteristic we may wish to estimate is the probability that a
particular component in an observation caused the system failure.

We wish to use as much information as possible to do this estimation.
We consider three cases:

1. We have masked data `md` with candidate sets and system failure times and seek
   to estimate the node failure probabilities of observations in this data.
   This case provides the most accurate estimates of the node probability failures,
   as have both system failure times and candidate sets as predictors of the node
   failure.

2. We have a new observation of a system failure time and an estimate of $\theta$
   from `md`. In this case, we cannot condition on candidate sets, since the
   observation does not include that information. However, we do have a system
   failure time.
   
3. We have an estimate of $\v\theta$ from `md` but wish to predict the node
   failure of a system that has failed, but we do not know when it failed.

We consider case 1 described above where we have masked data `md` that includes
both candidate sets and system failure times.

In this case, we are interested in
$$
    f_{K_i|C_i,T_i}(j|c_i,t_i,\v\theta) = \frac{h_j(t;\v{\theta_k})}{\sum_{j' \in c_i} h_{j'}(t_i;\v{\theta_{j'}})},
$$
which in the exponential series case simplifies to
$$
    f_{K_i|C_,T_i}(j|c_i,t_i,\v\theta) = \frac{\v{\theta_j}}{\sum_{j' \in c_i} \v{\theta_{j'}}}.
$$

We decorate `md` with this probability distribution with the decorator function
`md_series_component_failure_probability`, which accepts masked data as input
and returns the masked data with columns for component cause of failure
probabilities given by `k1`,...,`km`.

```{r}
#h <- list(function(t) theta.hat[1],
#          function(t) theta.hat[2],
#          function(t) theta.hat[3])
#md %>% md_series_component_failure_probability_decorator(h)
```

We notice that every row over the columns `k1`, `k2`, and `k3`
given a specific candidate set are the same.
This is as expected, since in the case of the exponential series,
the component failure rates are constant with respect to system
failure time.

If we already had an estimate of $\v\theta$ and we sought to predict
the failed components from only system lifetime data, we would just let
the candidate sets contain all of the component indexes.

Also, observe that the component failure probabilities
$$
    \hat{\v k}_i(\v\theta) = (\hat{k}_1,\hat{k}_2,\hat{k}_3)'
$$
is a random vector whose sampling distribution under the right conditions is a
multivariate normal whose $j$\textsuperscript{th} component is given by
$$
    \hat{k}_j \sim \mathcal{N}(f_{K_i|T_i}(j|t_i,\v\theta),\v\Sigma_i).
$$
We can simulate $n$ draws from $\hat{\v\theta}$ and then apply the above statistic of
interest, generating the data
$$
    \hat{\v k}^{(1)},\ldots,\hat{\v k}^{(n)}.
$$
